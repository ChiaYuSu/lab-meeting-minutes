###### tags: `NTU`、`NTUST`
<details>
<summary>Outline</summary>

- [Lab Meeting -- 2020.09.09](#lab-meeting----20200909)
- [論文](#論文)
  - [題目](#題目)
  - [論文大綱](#論文大綱)
  - [研究方法](#研究方法)
    - [欄位介紹](#欄位介紹)
    - [類型介紹](#類型介紹)
  - [結果](#結果)
    - [實驗結果一：本文使用的方法圖形特徵跟傳統特徵做分析比較](#實驗結果一本文使用的方法圖形特徵跟傳統特徵做分析比較)
    - [實驗結果二：將圖形特徵與傳統特徵結合](#實驗結果二將圖形特徵與傳統特徵結合)
    - [實驗結果三：單個特徵評估和特徵選擇](#實驗結果三單個特徵評估和特徵選擇)
    - [實驗結果四：使用基於非機器學習的技術進行性能測試](#實驗結果四使用基於非機器學習的技術進行性能測試)
  - [參考文獻](#參考文獻)
  - [所有資料存放處](#所有資料存放處)
</details>

# Lab Meeting -- 2020.09.09
- 時　間：2020/9/9（三）
- 主持人：陳俊良 教授
- 主　題：台大計畫開會

# 論文
## 題目
- 英文題目：[A graph-theoretic approach for the detection of phishing webpages]()
- 中文題目：基於圖論的觀點去檢測釣魚網站

## 論文大綱
<img src="1090909\1090909.003.jpeg" width="550px" />

1. [研究方法（Methodology）]()
   1. 討論釣魚網站，page 跟 page 之間的超連結模式研究
   2. 本文所建議的架構
   3. 本文以圖論觀點所提出的 17 項特徵
2. [實驗結果（Results）]()：那本文最終做了四個實驗，今天為了節省時間只講最主要也最重要的
   1. 實驗結果一：圖形特徵的性能基本測試結果
   2. 實驗結果二：將圖形特徵與傳統特徵相結合的測試結果
   3. 實驗結果三：單個特徵評估和特徵選擇
   4. 實驗結果四：與基於非機器學習技術的性能基準的測試結果

## 研究方法
<img src="1090909\1090909.005.jpeg" width="550px" />

那在實驗之前，首先要先對釣魚網站的超連結模式研究。

1. 在這篇 Paper 中一共提出了 17 個以圖論觀點衍生出來的圖形特徵。
2. 一般來說，決定釣魚網頁攻擊是否成功的主要設計目標有兩個，即可信度和躲避釣魚檢測器的能力。那為了實現這些設計目標，網路釣魚者傾向於採用各種超連結操縱模式，其中每種模式都需要花費不同的實施成本，所以在這個章節中，我們要討論網路釣魚網站中發現的三種最常見的超連結操縱模式。

---
<img src="1090909\1090909.006.jpeg" width="550px" />

### 欄位介紹
那先帶大家看一下這個表格的欄位標題：
1. 第一欄代表的是類型，一共有三種
2. 第二欄代表的是超連結的操縱模式
3. 第三欄代表的是實施的成本
4. 第四欄切成兩個欄位，第一個是可信度，第二個是逃避網路釣魚檢測器的能力
  1. 在可信度的部分，我們可以直接想像與目標合法網頁相比，網路釣魚網頁在外觀和行為方面的相似程度，那這篇文章研究的是正常網站與釣魚網站，網頁與網頁之間的連結關係
  2. 那在逃避網路釣魚檢測器的能力，簡單來說就是通過損害可信度的設計目標來做出回應，以逃避網絡釣魚檢測器的能力

### 類型介紹
1. 那首先第一個類型是，指向合法網站中的網頁的超鏈接
2. 指向同一釣魚域名中不存在的網頁的超連結
3. 空的 (沒有用的) 超連結重定向到相同釣魚域名中相同登入的網頁 (有點像是 self-loop 的概念)	

那通常，使用同一網域超鏈接類型 (即上表中的 Type-2 和 Type-3) 可能會欺騙以內部和外部超連結的差異為目標的一些釣魚檢測器工具。但是，當用戶注意到看上去不太熟悉的域名，斷開的連結或是「錯誤 404」頁面的超連結時，這可能會引起用戶的懷疑。

另一方面，通常大多數用戶仍依賴於瀏覽器中的本機安全機制 (即黑名單檢測)，某些網路釣魚者可能會認為選擇超連結操縱的 Type-1 方法是有利的。所以，受超鏈接操縱模式的啟發，本文提出了一系列圖形特徵，等一下後面會跟大家做介紹。

---
<img src="1090909\1090909.007.jpeg" width="550px" />

那接下來是本文所提出的架構：
第一階段涉及頁面連結數據的收集，而第二階段則需要構建 Web 圖形並提取圖形特徵，然後在最後階段，將圖形特徵輸入分類算法中，以區分網路釣魚或是合法網頁。

那這篇 Paper 主要的重點在第二階段還有第三階段，今天會從第二階段開始做介紹。

---
<img src="1090909\1090909.008.jpeg" width="550px" />

那第二階段是構建 Web 圖形並提取可用的圖形特徵。

首先在開始之前先簡單的把圖論的定義跟大家介紹一下：
1. Source Vertex: 源節點
2. Destination Vertex: 目標節點
3. Directed Edge: 有向性的邊
4. Self-loop: 自環 (也就是 Source Vertex 跟 Destination Vertex 是同一個節點)

---
<img src="1090909\1090909.009.jpeg" width="550px" />

那接下來還是定義的部分，這邊是以層數的概念去定義 L0 - L3：
1. L0: 第 0 層，也就是我們要查詢這個網頁是釣魚網站還是非釣魚網站的那個頁面
2. L1: 跟 Query Webpage 連接的第一階鄰居網頁
3. L2: 由 L1 再向外連接的第二階鄰居網頁

---
<img src="1090909\1090909.010.jpeg" width="550px" />

接下來，基於域和超連結關係，應用了基於規則的方法用顏色標記的方式去標記每個節點。
1. L0 的節點會被標記為紅色
2. L1 還有 L2 如果跟 L0 是同個網域的話也會被標記為紅色
3. 那其餘不同網域的 L1 會被標記為藍色
4. 不同網域的 L2 會被標記為綠色

那到這裡，算是完成網站建構跟標記，接下來跟大家介紹本文所提出的 17 個特徵。

---
<img src="1090909\1090909.011.jpeg" width="550px" />

那首先，F1 - F4 是一組的：
1. F1: 計算紅色頂點的百分比
2. F2、F3: 那 F2、F3 公式一樣，只是是計算藍色跟綠色頂點的百分比
3. F4: 扣掉查詢網頁頁面並計算紅色頂點的百分比
  
那本文提出這四項特徵的理由是，紅色、藍色和綠色頂點的百分比可以指示網路是由跟查詢網頁相同網域的網頁主導，還是由外部網域的網頁主導。

紅色頂點的百分比很高，也代表查詢網頁連接到同一網域中的許多活動網頁，通常由完善的合法網站所展現的特徵。那相反的，如果是有較高百分比的藍色頂點，就表明查詢網頁連接到許多外部網頁，這可能是表 1 中討論的 Type-1 超連結操縱方式，簡單來說就是釣魚網站可能會在網站中放入部分正常網頁的 url，來增加釣魚網站的可信度。最後，綠色頂點是離查詢網頁相對遙遠的網頁，不是由查詢網頁直接連接的，在這些網頁中，如果有過多的綠色頂點也要保持懷疑。

---
<img src="1090909\1090909.012.jpeg" width="550px" />

接下來是特徵 F5，那這邊有一個新的符號 h<sub>v</sub>，代表意思是所有頂點 v 上存在的 HTML 網頁集。

那 F5 特徵的公式就如投影片所表示的這個公式 V<sub>s</sub><sup>*</sup> ，也就是剛剛所提到的 F4 特徵，扣掉查詢網頁頁面並計算紅色頂點的數量，那這邊 F5 特徵會再扣掉空的、不存在的網頁超連結頁面。

那選擇這個特徵的理由是，有時釣魚網頁可能包含誤導性或無效的超鏈接。換句話說，在釣魚網頁中的任何給定超鏈接 url 都不存在實際的 HTML 文件。釣魚者利用這種技術在同一網頁上扣留用戶，直到獲得他們想要獲得的敏感訊息。

---
<img src="1090909\1090909.013.jpeg" width="550px" />

接下來，F6 - F8 是以頂點跟邊的觀點所提出的三項特徵：
1. F6: 指向查詢頁面節點的數量
2. F7: 從查詢頁面節點指出的數量
3. F8: 與 F6 公式相同，但扣除 self-loop 的部分

在網路釣魚檢測的中，很多人通常會拿 in_degree 來做分析，因為它可以代表特定頂點的可信度。那舉例來說，具有高 in_degree 值的查詢網頁被認為是更可信賴的，因為它被其他網頁大量引用。

---
<img src="1090909\1090909.014.jpeg" width="550px" />

F9、F10: 對於每個頂點，in_degree_centrality 跟 out_degree_centrality 分子的話都是計算連接到其傳入邊的頂點數或傳出邊的頂點數，然後，分母的話是在對每個頂點做歸一化的動作 (就是讓值在 0 - 1 中間)，最後再除以頂點總數。

那這兩個特徵使用理由也跟剛剛的 F6 - F8 特徵是相似的，如果一個網站的 in_degree_centrality 的值較大的話，我們也可以認為這個網站的重要性相對較高。

---
<img src="1090909\1090909.015.jpeg" width="550px" />

1. F11: 計算圖像密度
2. F12: 這個特跟 F11 是使用相同的公式，但是只針對被標記為紅色的頂點去做計算。

那會使用這兩個特徵的理由是，一般來說，與合法網頁相比，釣魚網頁中的超連結通常不會很講究，釣魚者的主要目的是想要獲取敏感資料，因此他們不大可能跟合法網頁一樣，投入大量的時間和精力去建立完整的架構。因此，一個密度較高的圖可能表明查詢網頁是偏向非釣魚式的。

---
<img src="1090909\1090909.016.jpeg" width="550px" />

F13 中文意思是中介中心性，主要用於衡量一個頂點在圖承擔「橋梁」角色的程度。中介中心性用於衡量一個頂點出現在其他任意兩個頂點對之間的最短路徑的次數，也就是說，如果一個頂點出現在任意兩個頂點間最短路徑的次數越多，那麽該頂點的中介中心性就越大。

---
<img src="1090909\1090909.017.jpeg" width="550px" />

F14 特徵是一種二進制特徵，用於檢查是否可以從圖中的其他網頁訪問查詢網頁，它是從特徵 F6 得出的。

那提出這個特徵的理由是，我們假設如果查詢網頁被其他網頁頂點連接，那麼查詢網頁的可信度將得到增強。那對於網路釣魚者而言，建立完整的網路架構所花費的時間和精力 CP 值是比較低的，相比之下合法網站會比較注重用戶的直觀性和頁面導航的便利性，因此提出這個特徵。

F15 跟 F14 一樣是一種二進制的特徵，用於檢查圖中任兩點是否雙向的。

那使用這個特徵的理由是，使用超連結操縱的 Type-2 和 Type-3 的網路釣魚者通常會花費最少的精力來製作網頁之間的工作超鏈接，因為 CP 值很低。那如果使用超連結操縱的 Type-1，則半連接可能性為零，因為合法網頁永遠不會連接回釣魚網站。

---
<img src="1090909\1090909.018.jpeg" width="550px" />

最後是 F16 跟 F17

1. F16: 中文翻譯是強連接組件，特徵的定義是在強連接組件中我們可以從任何頂點到達任何頂點，且需要具有雙向性
2. F17: 今天如果加入一個新的頂點進入組件中，這個組件是否還是可以達成強連接性，如果可以那這個組件就是 attracting_components

那使用這個特徵的理由是正常合法的網站會投入相對較多的時間來維護網頁之間的連接性，但是合法網頁不一定要連接到同一網域中所有的其他網頁（就是一對一的關係），而比較常見的現象是大部分的合法網頁都是，首頁會連到第二層網頁，然後第二層網頁底下可能還會有很多子網頁，但通常這些子網頁都不會連回首頁，而是回到第二層網頁後，經由第二層網頁回到首頁（也就是所謂的一對多的關係）。

相反的釣魚網站通常連一對一的關係都沒有做好。

## 結果
<img src="1090909\1090909.020.jpeg" width="550px" />

那本文用到的資料集是：
1. PhishTank 和 OpenPhish: 500 phishing URLs
2. Alexa 和 Common Crawl: 500 legitimate URLs

那蒐集的方式，是透過 Python 腳本工具來完成的。

---
### 實驗結果一：本文使用的方法圖形特徵跟傳統特徵做分析比較
<img src="1090909\1090909.021.jpeg" width="550px" />

Table 3. 是本文使用的方法、Table 4. 是傳統方法。

1. Table 3. 結果表明，當使用 C4.5 時，圖形特徵分別實現了 96.6％ 和 99.0％ 的高 TP Rate 及高 TN Rate，那在 Accuracy 數據上，97.8％ 的分數也表明，圖形功能確實可以有效地區分網路釣魚以及合法網頁
2. 不出所料，與 C4.5 和 Random Forest 相比，在 SVM 和 Naive Bayes 上的結果表現較差，這個現象也與其他多篇論文實驗結果一致
3. 當使用相對較高的性能分類器時 (例如 C4.5 和 Random Forest) 時，所提出的圖形特徵實現了相對穩定的 Accuracy
4. 那如果使用 C4.5 及 Random Forest 與傳統方法做比較，從 Table 3. 及 Table 4. 做比較，本文所提出的方法是優於傳統方法的

---
### 實驗結果二：將圖形特徵與傳統特徵結合
<img src="1090909\1090909.022.jpeg" width="550px" />

Table 3. 是本文使用的方法、Table 5. 是本文使用的特徵與傳統提出的特徵做結合。

該評估的目的是調查所提出的圖形特徵是否需要其他特徵來補充其可能的性能缺陷。

1. 將傳統方法所提出的特徵添加到圖形特徵後，Accuracy 在 Random Forest 上略微提高了 0.3％，在 C4.5 上降低了 0.2％
2. 當使用 SVM 對組合特徵進行評估時，雖然其 TN Rate 下降了 2.2％，但 Accuracy 卻提高了 8.6％，所以結論是可以通過增加傳統特徵來提高在 SVM 分類器下的 Accuracy
3. 當使用 Naive Bayes 對組合特徵進行評估時，在 Accuracy 上有顯著的的性能提升，提升了 4.1％。那這些實驗表明，Naive Bayes 和 SVM 可能需要大量特徵來獲得更好的 Accuracy

---
### 實驗結果三：單個特徵評估和特徵選擇
<img src="1090909\1090909.023.jpeg" width="550px" />

Table 6. 通過消除單個特徵來改變 Accuracy

為了確定每個圖形特徵對 Accuracy 的影響，我們通過一次消除一個特徵來進行分類。我們期待的是，如果某個特定的圖形特徵對整體性能有重大影響，那麼從數據集中刪除該特徵後，我們會看到 Accuracy 大幅下降。

1. 當取消特徵 F14 時，SVM 的性能下降最為明顯
2. Naive Bayes 的性能不太穩定，當取消特徵 F5、F6 和 F8 時，性能會大幅下降
3. C4.5 及 Random Forest，沒有觀察到明顯的性能下降，表明這兩個分類器都很強大

---
<img src="1090909\1090909.024.jpeg" width="550px" />

那之後他們還做了一個實驗，為了更好地了解哪些特徵是重要的，本文採用了特徵選擇算法，以獲得一個縮小的特徵集，以便進一步評估。最後從初步的圖形特徵 17 個進行了微調，從而得到了 Table. 7 中列出的 6 個特徵的縮減集合。

---
<img src="1090909\1090909.025.jpeg" width="550px" />

1. 在 SVM 上使用的減少的圖形特徵的 Accuracy 下降了 5.0％
2. 在 Naive Bayes 上表現良好，Accuracy 略有提高 1.8％，這也意味著縮減特徵集有助於提升 Naive Bayes 的性能
3. Random Forest 上縮減的圖形特徵的 Accuracy 保持不變，而使用 C4.5 時可實現 0.1％ 的小幅提高。因此，我們可以得出結論，在 C4.5 和 Random Forest 上利用特徵選擇能夠降低特徵維數，同時保持出色的 Accuracy

---
### 實驗結果四：使用基於非機器學習的技術進行性能測試
<img src="1090909\1090909.026.jpeg" width="550px" />

那最後一個實驗，本文將所提出的技術與基於非機器學習的技術進行了比較。文章中選擇了一種稱為 PhishWHO 的網路釣魚檢測模型，該模型被認為是基於非機器學習的技術中最先進的方法之一。

PhishWHO 利用加權 URL 令牌系統，從查詢網頁中提取身份關鍵字，並使用搜索引擎確定目標身份進行比較。

那本文所提出的技術的 Accuracy 高達 97.8％，明顯優於 PhishWHO。這進一步證實了所提出的基於圖的觀點能夠解決網絡釣魚威脅方面的有效性。

## 參考文獻
1. 待補, Retrieved from: 

## 所有資料存放處
- [Paper presentation / 1090909]()
- [Keynote]()
- [Keynote to PDF]()